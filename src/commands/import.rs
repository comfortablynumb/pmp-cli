use anyhow::{Context as AnyhowContext, Result};
use clap::{Args, Subcommand};
use std::path::{Path, PathBuf};

use crate::context::Context;
use crate::infrastructure::discovery::InfrastructureDiscovery;
use crate::infrastructure::{
    generate_required_providers_from_resources, validate_import, validate_schema_version,
    ConfigGenerator, DiscoveredResource, FileOrganization, ImportDestination, ImportWorkflow,
    ImportWorkflowOptions, ManualDiscovery, Provider, RollbackManager, SchemaVersionStatus,
    ValidationReport,
};

/// Import existing infrastructure into PMP
#[derive(Debug, Args)]
pub struct ImportCommand {
    #[command(subcommand)]
    subcommand: ImportSubcommand,
}

#[derive(Debug, Subcommand)]
enum ImportSubcommand {
    /// Import from pmp-cloud-inspector export file
    #[command(
        long_about = "Import infrastructure resources from a pmp-cloud-inspector JSON/YAML export file.\n\nThis command reads the export file generated by pmp-cloud-inspector and creates\nimport blocks for the discovered resources.\n\nExamples:\n  pmp import from-export ./cloud-inventory.json\n  pmp import from-export ./cloud-inventory.yaml --filter 'aws:ec2:*'\n  pmp import from-export ./export.json --provider aws --region us-east-1\n  pmp import from-export ./export.json --target-project my-infra --target-environment prod"
    )]
    FromExport(FromExportArgs),

    /// Discover resources from cloud provider APIs (deprecated)
    #[command(
        long_about = "Discover infrastructure resources from cloud provider APIs\n\nNOTE: This command is deprecated. Use 'pmp import from-export' instead,\nwhich imports resources discovered by pmp-cloud-inspector.\n\nExamples:\n  pmp import discover --provider aws --region us-east-1"
    )]
    Discover(InfraDiscoverArgs),

    /// Manually specify a resource to import
    #[command(
        long_about = "Import a resource by manually specifying type and ID\n\nUseful when you know the exact resource details.\n\nExamples:\n  pmp import manual aws_vpc vpc-12345 --name main-vpc\n  pmp import manual aws_s3_bucket my-bucket --provider aws\n  pmp import manual azurerm_resource_group /subscriptions/.../resourceGroups/my-rg"
    )]
    Manual(InfraManualArgs),

    /// Batch import from YAML configuration
    #[command(
        long_about = "Import multiple resources from a YAML configuration file\n\nExamples:\n  pmp import batch ./import-config.yaml\n  pmp import batch ./import-config.yaml --yes"
    )]
    Batch(InfraBatchArgs),
}

#[derive(Debug, Args)]
struct FromExportArgs {
    /// Path to the pmp-cloud-inspector export file (JSON or YAML)
    export_path: PathBuf,

    /// Filter by provider (aws, azure, gcp, github, etc.)
    #[arg(short, long)]
    provider: Option<String>,

    /// Filter by region
    #[arg(short, long)]
    region: Option<String>,

    /// Filter by resource type pattern (e.g., 'aws:ec2:*', 'github:*')
    #[arg(short, long)]
    filter: Option<String>,

    /// Filter by tags (key=value format, comma-separated)
    #[arg(long)]
    tags: Option<String>,

    /// Filter by account/subscription/project
    #[arg(long)]
    account: Option<String>,

    /// Target project name
    #[arg(long)]
    target_project: Option<String>,

    /// Target environment
    #[arg(long)]
    target_environment: Option<String>,

    /// Generate config files without running tofu commands
    #[arg(long)]
    generate_only: bool,

    /// Skip confirmation prompts
    #[arg(short, long)]
    yes: bool,

    /// Dry-run mode (show what would be imported)
    #[arg(long)]
    dry_run: bool,

    /// Maximum number of resources to import
    #[arg(long)]
    limit: Option<usize>,

    /// Show cost summary from export
    #[arg(long)]
    show_costs: bool,
}

#[derive(Debug, Args)]
struct InfraDiscoverArgs {
    /// Cloud provider (aws, azure, gcp)
    #[arg(short, long)]
    provider: String,

    /// Region to discover resources in
    #[arg(short, long)]
    region: Option<String>,

    /// Resource types to discover (comma-separated)
    #[arg(short = 't', long)]
    resource_types: Option<String>,

    /// Filter by tags (key=value format, comma-separated)
    #[arg(long)]
    tags: Option<String>,

    /// Azure subscription ID (required for Azure)
    #[arg(long)]
    subscription_id: Option<String>,

    /// GCP project ID (required for GCP)
    #[arg(long)]
    project_id: Option<String>,

    /// Target project name
    #[arg(long)]
    target_project: Option<String>,

    /// Target environment
    #[arg(long)]
    target_environment: Option<String>,

    /// Generate config files without running apply
    #[arg(long)]
    generate_only: bool,

    /// Skip confirmation prompts
    #[arg(short, long)]
    yes: bool,
}

#[derive(Debug, Args)]
struct InfraManualArgs {
    /// Resource type (e.g., aws_vpc, azurerm_resource_group)
    resource_type: String,

    /// Resource ID in the cloud provider
    resource_id: String,

    /// Cloud provider (inferred from resource type if not specified)
    #[arg(short, long)]
    provider: Option<String>,

    /// Terraform resource name (defaults to sanitized resource_id)
    #[arg(short, long)]
    name: Option<String>,

    /// Target project name
    #[arg(long)]
    target_project: Option<String>,

    /// Target environment
    #[arg(long)]
    target_environment: Option<String>,

    /// Generate config without running tofu commands
    #[arg(long)]
    generate_only: bool,

    /// Skip confirmation prompts
    #[arg(short, long)]
    yes: bool,
}

#[derive(Debug, Args)]
struct InfraBatchArgs {
    /// Path to YAML configuration file
    config_path: PathBuf,

    /// Skip confirmation prompts
    #[arg(short, long)]
    yes: bool,

    /// Dry-run mode
    #[arg(long)]
    dry_run: bool,
}

impl ImportCommand {
    pub fn execute(self, ctx: &Context) -> Result<()> {
        match self.subcommand {
            ImportSubcommand::FromExport(args) => Self::from_export(ctx, args),
            ImportSubcommand::Discover(args) => Self::discover(ctx, args),
            ImportSubcommand::Manual(args) => Self::manual(ctx, args),
            ImportSubcommand::Batch(args) => Self::batch(ctx, args),
        }
    }

    fn from_export(ctx: &Context, args: FromExportArgs) -> Result<()> {
        ctx.output.section("Import from Cloud Inspector Export");

        if !args.export_path.exists() {
            return Err(anyhow::anyhow!(
                "Export file not found: {}",
                args.export_path.display()
            ));
        }

        ctx.output.info(&format!(
            "Loading export: {}",
            args.export_path.display()
        ));

        // Read and parse export file
        let export = load_cloud_inspector_export(ctx, &args.export_path)?;

        // Validate schema version
        match validate_schema_version(export.schema_version.as_deref()) {
            SchemaVersionStatus::Valid => {}
            SchemaVersionStatus::Missing => {
                ctx.output.warning(
                    "Export file missing schema_version - assuming compatible format",
                );
            }
            SchemaVersionStatus::Newer(version) => {
                ctx.output.warning(&format!(
                    "Export schema version {} is newer than supported - some features may not work",
                    version
                ));
            }
            SchemaVersionStatus::TooOld(version) => {
                return Err(anyhow::anyhow!(
                    "Export schema version {} is too old. Please regenerate with a newer version of pmp-cloud-inspector",
                    version
                ));
            }
            SchemaVersionStatus::Invalid(version) => {
                return Err(anyhow::anyhow!(
                    "Invalid schema version format: '{}'. Expected semantic version (e.g., 1.0.0)",
                    version
                ));
            }
        }

        // Display metadata summary
        display_export_summary(ctx, &export, args.show_costs);

        // Build filter from args
        let filter = build_import_filter(&args)?;

        // Filter resources
        let filtered_resources: Vec<_> = export
            .resources
            .iter()
            .filter(|r| filter.matches(r))
            .take(args.limit.unwrap_or(usize::MAX))
            .collect();

        if filtered_resources.is_empty() {
            ctx.output.warning("No resources match the specified filters");
            return Ok(());
        }

        ctx.output.blank();
        ctx.output.subsection(&format!(
            "Resources to Import ({} of {})",
            filtered_resources.len(),
            export.resources.len()
        ));

        // Group by provider for display
        let mut by_provider: std::collections::HashMap<String, Vec<_>> =
            std::collections::HashMap::new();

        for resource in &filtered_resources {
            by_provider
                .entry(resource.provider.to_string())
                .or_default()
                .push(resource);
        }

        for (provider, resources) in &by_provider {
            ctx.output.info(&format!("  {} ({} resources):", provider, resources.len()));

            for resource in resources.iter().take(10) {
                let tf_type = crate::infrastructure::resource_mapper::map_resource_type(
                    &resource.resource_type,
                );
                let tf_type_str = tf_type
                    .map(|t| t.tf_type.to_string())
                    .unwrap_or_else(|| format!("(unsupported: {})", resource.resource_type));

                ctx.output.info(&format!(
                    "    - {} ({}) -> {}",
                    resource.name, resource.id, tf_type_str
                ));
            }

            if resources.len() > 10 {
                ctx.output.dimmed(&format!(
                    "    ... and {} more",
                    resources.len() - 10
                ));
            }
        }

        // Check for unsupported resource types
        let unsupported: Vec<_> = filtered_resources
            .iter()
            .filter(|r| {
                crate::infrastructure::resource_mapper::map_resource_type(&r.resource_type)
                    .is_none()
            })
            .collect();

        if !unsupported.is_empty() {
            ctx.output.blank();
            ctx.output.warning(&format!(
                "{} resource(s) have unsupported types and will be skipped:",
                unsupported.len()
            ));

            let mut unsupported_types: std::collections::HashSet<_> = std::collections::HashSet::new();

            for resource in &unsupported {
                unsupported_types.insert(&resource.resource_type);
            }

            for t in unsupported_types {
                ctx.output.dimmed(&format!("  - {}", t));
            }
        }

        // Filter to only supported resources
        let supported_resources: Vec<_> = filtered_resources
            .into_iter()
            .filter(|r| {
                crate::infrastructure::resource_mapper::map_resource_type(&r.resource_type)
                    .is_some()
            })
            .collect();

        if supported_resources.is_empty() {
            ctx.output.error("No supported resources to import");
            return Ok(());
        }

        if args.dry_run {
            ctx.output.blank();
            ctx.output.info("Dry-run mode: no changes will be made");
            return Ok(());
        }

        // Get destination
        let destination = get_import_destination(
            ctx,
            &args.target_project,
            &args.target_environment,
        )?;
        let project_path = get_project_path(ctx, &destination)?;

        ctx.output.blank();
        ctx.output.key_value("Target Project", destination.project_name());
        ctx.output.key_value("Environment", destination.environment());
        ctx.output.key_value("Path", &project_path.display().to_string());

        // Confirm
        if !args.yes {
            ctx.output.blank();

            if !ctx.input.confirm(
                &format!("Import {} resources?", supported_resources.len()),
                Some(true),
            )? {
                ctx.output.info("Import cancelled");
                return Ok(());
            }
        }

        // Convert to DiscoveredResource
        let discovered_resources = convert_to_discovered_resources(&supported_resources)?;

        // Validate before writing files
        let validation_report = validate_import(&discovered_resources, &project_path);

        if !display_validation_report(ctx, &validation_report) {
            return Err(anyhow::anyhow!("Import validation failed"));
        }

        // Initialize rollback manager
        let mut rollback = RollbackManager::new();

        // Create project directory if needed
        if destination.is_new_project() {
            ctx.fs.create_dir_all(&project_path)?;
            rollback.track_dir(project_path.clone());
            create_project_files(ctx, &project_path, &destination)?;
        }

        // Generate import blocks
        ctx.output.blank();
        ctx.output.subsection("Generating Import Blocks");

        let generator = ConfigGenerator::with_defaults();
        let import_block = generator.generate_import_blocks(&discovered_resources)?;

        let imports_file = project_path.join("_imports.tf");
        ctx.fs.write(&imports_file, &import_block)?;
        rollback.track_file(imports_file.clone());
        ctx.output.success(&format!("Created: {}", imports_file.display()));

        // Generate required_providers block
        let providers_result = generate_required_providers_from_resources(&discovered_resources);
        let providers_file = project_path.join("_providers.tf");
        ctx.fs.write(&providers_file, &providers_result.content)?;
        rollback.track_file(providers_file.clone());
        ctx.output.success(&format!("Created: {}", providers_file.display()));

        // Warn about unsupported providers
        for provider in &providers_result.unsupported_providers {
            ctx.output.warning(&format!(
                "Provider '{}' has no known Terraform source - add manually to _providers.tf",
                provider
            ));
        }

        if args.generate_only {
            ctx.output.blank();
            ctx.output.info("Generated import blocks only (--generate-only)");
            ctx.output.info("Next steps:");
            ctx.output.info("  1. Run: tofu init");
            ctx.output.info("  2. Run: tofu plan -generate-config-out=generated_resources.tf");
            ctx.output.info("  3. Review generated config");
            ctx.output.info("  4. Run: tofu apply");
            return Ok(());
        }

        // Execute workflow
        let workflow_options = ImportWorkflowOptions {
            auto_detect_dependencies: true,
            non_interactive: args.yes,
            generate_config: true,
            file_organization: FileOrganization::ByResourceType,
            continue_on_error: true,
            run_init: true,
            run_plan: true,
            run_apply: false,
        };

        let workflow = ImportWorkflow::new(workflow_options, &*ctx.output);

        match workflow.execute(discovered_resources, &destination, &project_path) {
            Ok(result) => {
                // Clear rollback on success - files are committed
                rollback.clear();

                ctx.output.blank();

                if result.success {
                    ctx.output.success(&format!(
                        "Import preparation complete! {} resource(s) prepared.",
                        result.resource_results.len()
                    ));
                    ctx.output.blank();
                    ctx.output.info("Next steps:");
                    ctx.output.info("  1. Review generated config in generated_resources.tf");
                    ctx.output.info("  2. Run: tofu apply");
                } else {
                    let failed = result.failed_count();
                    ctx.output.warning(&format!(
                        "Import completed with {} failure(s)",
                        failed
                    ));
                }

                Ok(())
            }
            Err(e) => {
                // Roll back on failure
                if rollback.has_tracked_items() {
                    ctx.output.blank();
                    ctx.output.info("Rolling back created files...");
                    let rollback_result = rollback.rollback(&*ctx.fs);
                    ctx.output.info(&rollback_result.to_string());
                }

                Err(e.into())
            }
        }
    }

    fn discover(ctx: &Context, args: InfraDiscoverArgs) -> Result<()> {
        ctx.output.section("Infrastructure Discovery (Deprecated)");
        ctx.output.warning("This command is deprecated.");
        ctx.output.info("Use 'pmp import from-export' instead.");
        ctx.output.info("First, run pmp-cloud-inspector to discover resources,");
        ctx.output.info("then import the generated export file.");
        ctx.output.blank();
        ctx.output.section("Infrastructure Discovery");

        let provider = Provider::from_str(&args.provider).ok_or_else(|| {
            anyhow::anyhow!(
                "Unknown provider '{}'. Supported: aws, azure, gcp",
                args.provider
            )
        })?;

        ctx.output.info(&format!(
            "Provider: {} (API discovery not yet implemented)",
            provider.display_name()
        ));
        ctx.output.info("Use 'pmp import infrastructure manual' for now.");

        Ok(())
    }

    fn manual(ctx: &Context, args: InfraManualArgs) -> Result<()> {
        ctx.output.section("Manual Infrastructure Import");

        // Infer provider from resource type if not specified
        let provider = if let Some(ref p) = args.provider {
            Provider::from_str(p).ok_or_else(|| {
                anyhow::anyhow!("Unknown provider '{}'. Supported: aws, azure, gcp", p)
            })?
        } else {
            infer_provider_from_type(&args.resource_type)?
        };

        ctx.output.key_value("Provider", provider.display_name());
        ctx.output.key_value("Resource Type", &args.resource_type);
        ctx.output.key_value("Resource ID", &args.resource_id);

        // Create manual discovery and validate resource type
        let discovery = ManualDiscovery::new(provider);
        let supported_types = discovery.supported_resource_types();

        if !supported_types.contains(&args.resource_type.as_str()) {
            return Err(anyhow::anyhow!(
                "Unsupported resource type '{}' for provider {}.\n\
                 Supported types:\n  {}",
                args.resource_type,
                provider.display_name(),
                supported_types.join("\n  ")
            ));
        }

        // Create discovered resource
        let mut resource = DiscoveredResource::new(
            provider,
            args.resource_type.clone(),
            args.resource_id.clone(),
        );

        if let Some(name) = &args.name {
            resource = resource.with_name(name.clone());
        }

        ctx.output.blank();
        ctx.output.info(&format!(
            "Terraform name: {}",
            resource.suggested_tf_name()
        ));

        // Get destination
        let destination = get_import_destination(ctx, &args.target_project, &args.target_environment)?;
        let project_path = get_project_path(ctx, &destination)?;

        ctx.output.blank();
        ctx.output.key_value("Target Project", destination.project_name());
        ctx.output.key_value("Environment", destination.environment());
        ctx.output.key_value("Path", &project_path.display().to_string());

        // Generate import block
        ctx.output.blank();
        ctx.output.subsection("Generated Import Block");

        let generator = ConfigGenerator::with_defaults();
        let import_block = generator.generate_import_blocks(&[resource.clone()])?;
        ctx.output.info(&import_block);

        // Confirm
        if !args.yes
            && !args.generate_only
            && !ctx.input.confirm("\nProceed with import?", Some(true))?
        {
            ctx.output.info("Import cancelled");
            return Ok(());
        }

        // Validate before writing files
        let resources_vec = vec![resource.clone()];
        let validation_report = validate_import(&resources_vec, &project_path);

        if !display_validation_report(ctx, &validation_report) {
            return Err(anyhow::anyhow!("Import validation failed"));
        }

        // Initialize rollback manager
        let mut rollback = RollbackManager::new();

        // Create project directory if needed
        if destination.is_new_project() {
            ctx.fs.create_dir_all(&project_path)?;
            rollback.track_dir(project_path.clone());
            create_project_files(ctx, &project_path, &destination)?;
        }

        // Write import block
        let imports_file = project_path.join("_imports.tf");
        ctx.fs.write(&imports_file, &import_block)?;
        rollback.track_file(imports_file.clone());
        ctx.output.success(&format!("Created: {}", imports_file.display()));

        // Generate required_providers block
        let providers_result = generate_required_providers_from_resources(&resources_vec);
        let providers_file = project_path.join("_providers.tf");
        ctx.fs.write(&providers_file, &providers_result.content)?;
        rollback.track_file(providers_file.clone());
        ctx.output.success(&format!("Created: {}", providers_file.display()));

        // Warn about unsupported providers
        for provider_name in &providers_result.unsupported_providers {
            ctx.output.warning(&format!(
                "Provider '{}' has no known Terraform source - add manually to _providers.tf",
                provider_name
            ));
        }

        if args.generate_only {
            ctx.output.blank();
            ctx.output.info("Generated import block only (--generate-only)");
            ctx.output.info("Next steps:");
            ctx.output.info("  1. Run: tofu init");
            ctx.output.info("  2. Run: tofu plan -generate-config-out=generated_resources.tf");
            ctx.output.info("  3. Review generated config");
            ctx.output.info("  4. Run: tofu apply");
            return Ok(());
        }

        // Execute workflow
        let workflow_options = ImportWorkflowOptions {
            auto_detect_dependencies: false,
            non_interactive: args.yes,
            generate_config: true,
            file_organization: FileOrganization::SingleFile,
            continue_on_error: false,
            run_init: true,
            run_plan: true,
            run_apply: false,
        };

        let workflow = ImportWorkflow::new(workflow_options, &*ctx.output);

        match workflow.execute(resources_vec, &destination, &project_path) {
            Ok(result) => {
                // Clear rollback on success
                rollback.clear();

                if result.success {
                    ctx.output.blank();
                    ctx.output.success("Import preparation complete!");
                    ctx.output.info("\nNext steps:");
                    ctx.output.info("  1. Review generated config in generated_resources.tf");
                    ctx.output.info("  2. Run: tofu apply");
                }

                Ok(())
            }
            Err(e) => {
                // Roll back on failure
                if rollback.has_tracked_items() {
                    ctx.output.blank();
                    ctx.output.info("Rolling back created files...");
                    let rollback_result = rollback.rollback(&*ctx.fs);
                    ctx.output.info(&rollback_result.to_string());
                }

                Err(e.into())
            }
        }
    }

    fn batch(ctx: &Context, args: InfraBatchArgs) -> Result<()> {
        ctx.output.section("Batch Infrastructure Import");

        if !args.config_path.exists() {
            return Err(anyhow::anyhow!(
                "Config file not found: {}",
                args.config_path.display()
            ));
        }

        ctx.output.info(&format!(
            "Loading config: {}",
            args.config_path.display()
        ));

        let config_content = ctx.fs.read_to_string(&args.config_path)?;
        let config: InfraBatchConfig = serde_yaml::from_str(&config_content)
            .with_context(|| "Failed to parse batch import configuration")?;

        // Validate config
        validate_batch_config(&config)?;

        ctx.output.info(&format!(
            "Found {} resource(s) to import",
            config.spec.resources.len()
        ));

        // Parse provider
        let provider = Provider::from_str(&config.spec.provider).ok_or_else(|| {
            anyhow::anyhow!(
                "Unknown provider '{}'. Supported: aws, azure, gcp",
                config.spec.provider
            )
        })?;

        // Validate all resource types
        let discovery = ManualDiscovery::new(provider);
        let supported_types = discovery.supported_resource_types();
        let mut invalid_types = Vec::new();

        for resource in &config.spec.resources {
            if !supported_types.contains(&resource.resource_type.as_str()) {
                invalid_types.push(resource.resource_type.clone());
            }
        }

        if !invalid_types.is_empty() {
            return Err(anyhow::anyhow!(
                "Unsupported resource types for provider {}:\n  {}\n\n\
                 Supported types:\n  {}",
                provider.display_name(),
                invalid_types.join("\n  "),
                supported_types.join("\n  ")
            ));
        }

        // Display resources to import
        ctx.output.blank();
        ctx.output.subsection("Resources to Import");

        for resource in &config.spec.resources {
            let name_str = resource
                .name
                .as_ref()
                .map(|n| format!(" ({})", n))
                .unwrap_or_default();
            ctx.output.info(&format!(
                "  - {} {}{}", resource.resource_type, resource.id, name_str
            ));
        }

        // Display destination
        ctx.output.blank();
        ctx.output.key_value("Destination", &config.spec.destination.dest_type);
        ctx.output.key_value("Project", &config.spec.destination.project_name);
        ctx.output.key_value("Environment", &config.spec.destination.environment);

        if args.dry_run {
            ctx.output.blank();
            ctx.output.info("Dry-run mode: no changes will be made");
            return Ok(());
        }

        // Confirm
        if !args.yes {
            ctx.output.blank();

            if !ctx.input.confirm("Proceed with batch import?", Some(true))? {
                ctx.output.info("Import cancelled");
                return Ok(());
            }
        }

        // Build destination
        let destination = build_batch_destination(&config.spec.destination)?;
        let project_path = get_project_path(ctx, &destination)?;

        // Convert batch resources to DiscoveredResources
        let resources = build_discovered_resources(&config, provider, &discovery)?;

        // Validate before writing files
        let validation_report = validate_import(&resources, &project_path);

        if !display_validation_report(ctx, &validation_report) {
            return Err(anyhow::anyhow!("Import validation failed"));
        }

        // Initialize rollback manager
        let mut rollback = RollbackManager::new();

        // Create project directory if needed
        if destination.is_new_project() {
            ctx.fs.create_dir_all(&project_path)?;
            rollback.track_dir(project_path.clone());
            create_project_files(ctx, &project_path, &destination)?;
        }

        // Generate and write import blocks
        ctx.output.blank();
        ctx.output.subsection("Generated Import Blocks");

        let generator = ConfigGenerator::with_defaults();
        let import_block = generator.generate_import_blocks(&resources)?;
        ctx.output.info(&import_block);

        let imports_file = project_path.join("_imports.tf");
        ctx.fs.write(&imports_file, &import_block)?;
        rollback.track_file(imports_file.clone());
        ctx.output.success(&format!("Created: {}", imports_file.display()));

        // Generate required_providers block
        let providers_result = generate_required_providers_from_resources(&resources);
        let providers_file = project_path.join("_providers.tf");
        ctx.fs.write(&providers_file, &providers_result.content)?;
        rollback.track_file(providers_file.clone());
        ctx.output.success(&format!("Created: {}", providers_file.display()));

        // Warn about unsupported providers
        for provider_name in &providers_result.unsupported_providers {
            ctx.output.warning(&format!(
                "Provider '{}' has no known Terraform source - add manually to _providers.tf",
                provider_name
            ));
        }

        // Execute workflow
        let file_organization = config
            .spec
            .options
            .file_organization
            .as_ref()
            .map(|s| match s.as_str() {
                "by_type" | "by_resource_type" => FileOrganization::ByResourceType,
                "by_resource" => FileOrganization::ByResource,
                _ => FileOrganization::SingleFile,
            })
            .unwrap_or(FileOrganization::SingleFile);

        let workflow_options = ImportWorkflowOptions {
            auto_detect_dependencies: config.spec.options.auto_detect_dependencies,
            non_interactive: args.yes,
            generate_config: config.spec.options.generate_config,
            file_organization,
            continue_on_error: true,
            run_init: true,
            run_plan: config.spec.options.generate_config,
            run_apply: false,
        };

        let workflow = ImportWorkflow::new(workflow_options, &*ctx.output);

        match workflow.execute(resources, &destination, &project_path) {
            Ok(result) => {
                // Clear rollback on success
                rollback.clear();

                // Display results
                ctx.output.blank();

                if result.success {
                    ctx.output.success(&format!(
                        "Batch import preparation complete! {} resource(s) prepared.",
                        result.resource_results.len()
                    ));
                    ctx.output.blank();
                    ctx.output.info("Next steps:");
                    ctx.output.info("  1. Review generated config in generated_resources.tf");
                    ctx.output.info("  2. Run: tofu apply");
                } else {
                    let failed = result.failed_count();
                    ctx.output.warning(&format!(
                        "Batch import completed with {} failure(s)",
                        failed
                    ));
                }

                Ok(())
            }
            Err(e) => {
                // Roll back on failure
                if rollback.has_tracked_items() {
                    ctx.output.blank();
                    ctx.output.info("Rolling back created files...");
                    let rollback_result = rollback.rollback(&*ctx.fs);
                    ctx.output.info(&rollback_result.to_string());
                }

                Err(e.into())
            }
        }
    }

}

// ============================================================================
// Import Helper Functions
// ============================================================================

/// Infer provider from resource type prefix
fn infer_provider_from_type(resource_type: &str) -> Result<Provider> {
    if resource_type.starts_with("aws_") {
        Ok(Provider::Aws)
    } else if resource_type.starts_with("azurerm_") {
        Ok(Provider::Azure)
    } else if resource_type.starts_with("google_") {
        Ok(Provider::Gcp)
    } else {
        Err(anyhow::anyhow!(
            "Cannot infer provider from resource type '{}'. \
             Use --provider to specify explicitly.",
            resource_type
        ))
    }
}

/// Display validation report and check if import should proceed
fn display_validation_report(ctx: &Context, report: &ValidationReport) -> bool {
    if !report.errors.is_empty() {
        ctx.output.blank();
        ctx.output.error(&format!("Validation failed with {} error(s):", report.errors.len()));

        for err in &report.errors {
            ctx.output.error(&format!("  - {}", err));
        }
    }

    if !report.warnings.is_empty() {
        ctx.output.blank();
        ctx.output.warning(&format!("Validation warnings ({}):", report.warnings.len()));

        for warn in &report.warnings {
            ctx.output.warning(&format!("  - {}", warn));
        }
    }

    report.is_valid()
}

/// Get import destination from args or prompt user
fn get_import_destination(
    ctx: &Context,
    target_project: &Option<String>,
    target_environment: &Option<String>,
) -> Result<ImportDestination> {
    // Check if we have an existing infrastructure
    let infra_root = crate::collection::CollectionDiscovery::find_collection(&*ctx.fs)?;

    let (project_name, environment, is_new) = if let Some(name) = target_project {
        let env = target_environment
            .clone()
            .unwrap_or_else(|| "production".to_string());

        // Check if project exists
        if let Some((_, root)) = &infra_root {
            let project_dir = root.join("projects").join(name);
            let env_dir = project_dir.join("environments").join(&env);

            if ctx.fs.exists(&env_dir) {
                (name.clone(), env, false)
            } else {
                (name.clone(), env, true)
            }
        } else {
            (name.clone(), env, true)
        }
    } else {
        // Prompt for project name
        let name = ctx.input.text("Project name", Some("imported-infra"))?;
        let env = target_environment
            .clone()
            .unwrap_or_else(|| "production".to_string());
        (name, env, true)
    };

    if is_new {
        Ok(ImportDestination::NewProject {
            project_name,
            environment,
        })
    } else {
        Ok(ImportDestination::ExistingProject {
            project_name,
            environment,
        })
    }
}

/// Get the file path for the import destination
fn get_project_path(ctx: &Context, destination: &ImportDestination) -> Result<PathBuf> {
    let infra_root = crate::collection::CollectionDiscovery::find_collection(&*ctx.fs)?;

    if let Some((_, root)) = infra_root {
        Ok(root
            .join("projects")
            .join(destination.project_name())
            .join("environments")
            .join(destination.environment()))
    } else {
        // No infrastructure, create in current directory
        let cwd = std::env::current_dir()?;
        Ok(cwd
            .join("projects")
            .join(destination.project_name())
            .join("environments")
            .join(destination.environment()))
    }
}

/// Create project files for a new project
fn create_project_files(
    ctx: &Context,
    env_path: &Path,
    destination: &ImportDestination,
) -> Result<()> {
    // Create parent directories
    ctx.fs.create_dir_all(env_path)?;

    // Get project directory (parent of environments/<env>)
    let project_dir = env_path
        .parent()
        .and_then(|p| p.parent())
        .ok_or_else(|| anyhow::anyhow!("Invalid project path"))?;

    // Create .pmp.project.yaml
    let project_yaml = format!(
        r#"apiVersion: pmp.io/v1
kind: Project
metadata:
  name: {}
  description: Imported infrastructure resources
"#,
        destination.project_name()
    );

    let project_file = project_dir.join(".pmp.project.yaml");

    if !ctx.fs.exists(&project_file) {
        ctx.fs.write(&project_file, &project_yaml)?;
        ctx.output.success(&format!("Created: {}", project_file.display()));
    }

    // Create .pmp.environment.yaml
    let env_yaml = format!(
        r#"apiVersion: pmp.io/v1
kind: ImportedInfrastructure
metadata:
  name: {}
  environment_name: {}
  description: Imported infrastructure resources
spec:
  dependencies: []
"#,
        destination.project_name(),
        destination.environment()
    );

    let env_file = env_path.join(".pmp.environment.yaml");

    if !ctx.fs.exists(&env_file) {
        ctx.fs.write(&env_file, &env_yaml)?;
        ctx.output.success(&format!("Created: {}", env_file.display()));
    }

    // Create _common.tf with local backend
    let common_tf = r#"# Generated by PMP infrastructure import

terraform {
  backend "local" {
    path = "terraform.tfstate"
  }
}
"#;

    let common_file = env_path.join("_common.tf");

    if !ctx.fs.exists(&common_file) {
        ctx.fs.write(&common_file, common_tf)?;
        ctx.output.success(&format!("Created: {}", common_file.display()));
    }

    Ok(())
}

// ============================================================================
// Infrastructure Batch Import Configuration
// ============================================================================

/// Batch infrastructure import configuration
#[derive(Debug, serde::Deserialize)]
#[serde(rename_all = "camelCase")]
#[allow(dead_code)]
struct InfraBatchConfig {
    api_version: String,
    kind: String,
    metadata: InfraBatchMetadata,
    spec: InfraBatchSpec,
}

#[derive(Debug, serde::Deserialize)]
#[allow(dead_code)]
struct InfraBatchMetadata {
    name: String,
}

#[derive(Debug, serde::Deserialize)]
#[serde(rename_all = "camelCase")]
#[allow(dead_code)]
struct InfraBatchSpec {
    provider: String,
    #[serde(default)]
    regions: Vec<String>,
    destination: InfraBatchDestination,
    resources: Vec<InfraBatchResource>,
    #[serde(default)]
    options: InfraBatchOptions,
}

#[derive(Debug, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
struct InfraBatchDestination {
    #[serde(rename = "type")]
    dest_type: String,
    project_name: String,
    environment: String,
}

#[derive(Debug, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
struct InfraBatchResource {
    #[serde(rename = "type")]
    resource_type: String,
    id: String,
    #[serde(default)]
    name: Option<String>,
}

#[derive(Debug, Default, serde::Deserialize)]
#[serde(rename_all = "snake_case")]
struct InfraBatchOptions {
    #[serde(default = "default_true")]
    generate_config: bool,
    #[serde(default = "default_true")]
    auto_detect_dependencies: bool,
    #[serde(default)]
    file_organization: Option<String>,
}

fn default_true() -> bool {
    true
}

/// Validate batch import configuration
fn validate_batch_config(config: &InfraBatchConfig) -> Result<()> {
    if config.api_version != "pmp.io/v1" {
        return Err(anyhow::anyhow!(
            "Unsupported apiVersion: {}. Expected: pmp.io/v1",
            config.api_version
        ));
    }

    if config.kind != "InfrastructureImport" {
        return Err(anyhow::anyhow!(
            "Invalid kind: {}. Expected: InfrastructureImport",
            config.kind
        ));
    }

    if config.spec.resources.is_empty() {
        return Err(anyhow::anyhow!("No resources specified in config"));
    }

    if config.spec.destination.project_name.is_empty() {
        return Err(anyhow::anyhow!("project_name is required in destination"));
    }

    if config.spec.destination.environment.is_empty() {
        return Err(anyhow::anyhow!("environment is required in destination"));
    }

    Ok(())
}

/// Build ImportDestination from batch config destination
fn build_batch_destination(dest: &InfraBatchDestination) -> Result<ImportDestination> {
    match dest.dest_type.as_str() {
        "new_project" => Ok(ImportDestination::NewProject {
            project_name: dest.project_name.clone(),
            environment: dest.environment.clone(),
        }),
        "existing_project" => Ok(ImportDestination::ExistingProject {
            project_name: dest.project_name.clone(),
            environment: dest.environment.clone(),
        }),
        other => Err(anyhow::anyhow!(
            "Invalid destination type: {}. Expected: new_project or existing_project",
            other
        )),
    }
}

/// Build DiscoveredResource list from batch config
fn build_discovered_resources(
    config: &InfraBatchConfig,
    provider: Provider,
    discovery: &ManualDiscovery,
) -> Result<Vec<DiscoveredResource>> {
    let mut resources = Vec::new();

    for batch_resource in &config.spec.resources {
        let mut resource = DiscoveredResource::new(
            provider,
            batch_resource.resource_type.clone(),
            batch_resource.id.clone(),
        );

        if let Some(name) = &batch_resource.name {
            resource = resource.with_name(name.clone());
        }

        // Detect dependencies based on resource type
        let deps = discovery.detect_dependencies(&resource)?;
        resource.dependencies = deps;

        resources.push(resource);
    }

    Ok(resources)
}

// ============================================================================
// Cloud Inspector Import Helper Functions
// ============================================================================

use crate::infrastructure::cloud_inspector::{
    CloudInspectorExport, CloudInspectorProvider, CloudInspectorResource, ImportFilter,
};

/// Load and parse a pmp-cloud-inspector export file (JSON or YAML)
fn load_cloud_inspector_export(
    ctx: &Context,
    path: &Path,
) -> Result<CloudInspectorExport> {
    let content = ctx.fs.read_to_string(path)?;

    // Determine format from extension
    let extension = path
        .extension()
        .and_then(|e| e.to_str())
        .unwrap_or("")
        .to_lowercase();

    let export: CloudInspectorExport = match extension.as_str() {
        "json" => serde_json::from_str(&content)
            .with_context(|| "Failed to parse JSON export file")?,
        "yaml" | "yml" => serde_yaml::from_str(&content)
            .with_context(|| "Failed to parse YAML export file")?,
        _ => {
            // Try JSON first, then YAML
            serde_json::from_str(&content).or_else(|_| {
                serde_yaml::from_str(&content)
                    .with_context(|| "Failed to parse export file (tried JSON and YAML)")
            })?
        }
    };

    Ok(export)
}

/// Display a summary of the export file
fn display_export_summary(
    ctx: &Context,
    export: &CloudInspectorExport,
    show_costs: bool,
) {
    ctx.output.blank();
    ctx.output.subsection("Export Summary");
    ctx.output.key_value("Timestamp", &export.metadata.timestamp);
    ctx.output.key_value("Total Resources", &export.metadata.total_count.to_string());

    // Show by provider
    if !export.metadata.by_provider.is_empty() {
        ctx.output.blank();
        ctx.output.info("Resources by Provider:");

        for (provider, count) in &export.metadata.by_provider {
            ctx.output.info(&format!("  {}: {}", provider, count));
        }
    }

    // Show by region
    if !export.metadata.by_region.is_empty() {
        ctx.output.blank();
        ctx.output.info("Resources by Region:");

        for (region, count) in &export.metadata.by_region {
            ctx.output.info(&format!("  {}: {}", region, count));
        }
    }

    // Show costs if requested
    if show_costs
        && let Some(cost) = &export.metadata.total_cost
    {
        ctx.output.blank();
        ctx.output.subsection("Cost Summary");

        if let Some(total) = cost.total {
            ctx.output.key_value(
                "Total Monthly Cost",
                &format!("{:.2} {}", total, cost.currency),
            );
        }

        if !cost.by_provider.is_empty() {
            ctx.output.info("Cost by Provider:");

            for (provider, amount) in &cost.by_provider {
                ctx.output.info(&format!(
                    "  {}: {:.2} {}",
                    provider, amount, cost.currency
                ));
            }
        }

        if !cost.by_type.is_empty() {
            ctx.output.info("Top Resource Types by Cost:");

            let mut costs: Vec<_> = cost.by_type.iter().collect();
            costs.sort_by(|a, b| b.1.partial_cmp(a.1).unwrap_or(std::cmp::Ordering::Equal));

            for (resource_type, amount) in costs.iter().take(5) {
                ctx.output.info(&format!(
                    "  {}: {:.2} {}",
                    resource_type, amount, cost.currency
                ));
            }
        }
    }
}

/// Build an ImportFilter from command-line arguments
fn build_import_filter(args: &FromExportArgs) -> Result<ImportFilter> {
    let mut filter = ImportFilter::new();

    // Filter by provider
    if let Some(provider_str) = &args.provider {
        let provider = match provider_str.to_lowercase().as_str() {
            "aws" => CloudInspectorProvider::Aws,
            "azure" | "azurerm" => CloudInspectorProvider::Azure,
            "gcp" | "google" => CloudInspectorProvider::Gcp,
            "github" => CloudInspectorProvider::Github,
            "gitlab" => CloudInspectorProvider::Gitlab,
            "jfrog" | "artifactory" => CloudInspectorProvider::Jfrog,
            "okta" => CloudInspectorProvider::Okta,
            "auth0" => CloudInspectorProvider::Auth0,
            "jira" => CloudInspectorProvider::Jira,
            "opsgenie" => CloudInspectorProvider::Opsgenie,
            other => {
                return Err(anyhow::anyhow!(
                    "Unknown provider '{}'. Supported: aws, azure, gcp, github, gitlab, jfrog, okta, auth0, jira, opsgenie",
                    other
                ));
            }
        };
        filter = filter.with_providers(vec![provider]);
    }

    // Filter by resource type pattern
    if let Some(pattern) = &args.filter {
        let patterns: Vec<String> = pattern
            .split(',')
            .map(|s| s.trim().to_string())
            .collect();
        filter = filter.with_type_patterns(patterns);
    }

    // Filter by region
    if let Some(region) = &args.region {
        let regions: Vec<String> = region
            .split(',')
            .map(|s| s.trim().to_string())
            .collect();
        filter = filter.with_regions(regions);
    }

    // Filter by tags
    if let Some(tags_str) = &args.tags {
        let mut tags = std::collections::HashMap::new();

        for pair in tags_str.split(',') {
            let parts: Vec<&str> = pair.split('=').collect();

            if parts.len() == 2 {
                tags.insert(
                    parts[0].trim().to_string(),
                    parts[1].trim().to_string(),
                );
            }
        }

        if !tags.is_empty() {
            filter = filter.with_tags(tags);
        }
    }

    // Filter by account
    if let Some(account) = &args.account {
        let accounts: Vec<String> = account
            .split(',')
            .map(|s| s.trim().to_string())
            .collect();
        filter.accounts = accounts;
    }

    // Limit
    if let Some(limit) = args.limit {
        filter = filter.with_limit(limit);
    }

    Ok(filter)
}

/// Convert cloud inspector resources to PMP DiscoveredResource
fn convert_to_discovered_resources(
    resources: &[&CloudInspectorResource],
) -> Result<Vec<DiscoveredResource>> {
    let mut result = Vec::new();

    for ci_resource in resources {
        let tf_type = crate::infrastructure::resource_mapper::map_resource_type(
            &ci_resource.resource_type,
        );

        if let Some(tf_type) = tf_type {
            let provider = crate::infrastructure::resource_mapper::extract_provider(
                &ci_resource.resource_type,
            )
            .ok_or_else(|| {
                anyhow::anyhow!(
                    "Failed to extract provider from type: {}",
                    ci_resource.resource_type
                )
            })?;

            let mut resource = DiscoveredResource::new(
                provider,
                tf_type.tf_type.to_string(),
                ci_resource.id.clone(),
            )
            .with_name(ci_resource.name.clone())
            .with_tags(ci_resource.tags.clone());

            if let Some(region) = &ci_resource.region {
                resource = resource.with_region(region.clone());
            }

            // Copy properties as attributes
            for (key, value) in &ci_resource.properties {
                resource = resource.with_attribute(key.clone(), value.clone());
            }

            // Convert relationships to dependencies
            resource.dependencies = crate::infrastructure::resource_mapper::convert_relationships(
                &ci_resource.relationships,
            );

            result.push(resource);
        }
    }

    Ok(result)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_infer_provider_from_type_aws() {
        let result = infer_provider_from_type("aws_vpc").unwrap();
        assert!(matches!(result, Provider::Aws));
    }

    #[test]
    fn test_infer_provider_from_type_azure() {
        let result = infer_provider_from_type("azurerm_resource_group").unwrap();
        assert!(matches!(result, Provider::Azure));
    }

    #[test]
    fn test_infer_provider_from_type_gcp() {
        let result = infer_provider_from_type("google_compute_instance").unwrap();
        assert!(matches!(result, Provider::Gcp));
    }

    #[test]
    fn test_infer_provider_from_type_unknown() {
        let result = infer_provider_from_type("unknown_resource");
        assert!(result.is_err());

        let err = result.unwrap_err();
        assert!(err.to_string().contains("Cannot infer provider"));
    }

    #[test]
    fn test_infer_provider_from_type_partial_prefix() {
        // Should not match partial prefixes
        let result = infer_provider_from_type("aw_something");
        assert!(result.is_err());
    }

    #[test]
    fn test_default_true() {
        assert!(default_true());
    }

    #[test]
    fn test_validate_batch_config_valid() {
        let config = InfraBatchConfig {
            api_version: "pmp.io/v1".to_string(),
            kind: "InfrastructureImport".to_string(),
            metadata: InfraBatchMetadata {
                name: "test-import".to_string(),
            },
            spec: InfraBatchSpec {
                provider: "aws".to_string(),
                regions: vec![],
                destination: InfraBatchDestination {
                    dest_type: "new_project".to_string(),
                    project_name: "my-project".to_string(),
                    environment: "production".to_string(),
                },
                resources: vec![InfraBatchResource {
                    resource_type: "aws_vpc".to_string(),
                    id: "vpc-12345".to_string(),
                    name: Some("main-vpc".to_string()),
                }],
                options: InfraBatchOptions::default(),
            },
        };

        let result = validate_batch_config(&config);
        assert!(result.is_ok());
    }

    #[test]
    fn test_validate_batch_config_invalid_api_version() {
        let config = InfraBatchConfig {
            api_version: "pmp.io/v2".to_string(),
            kind: "InfrastructureImport".to_string(),
            metadata: InfraBatchMetadata {
                name: "test".to_string(),
            },
            spec: InfraBatchSpec {
                provider: "aws".to_string(),
                regions: vec![],
                destination: InfraBatchDestination {
                    dest_type: "new_project".to_string(),
                    project_name: "my-project".to_string(),
                    environment: "production".to_string(),
                },
                resources: vec![InfraBatchResource {
                    resource_type: "aws_vpc".to_string(),
                    id: "vpc-12345".to_string(),
                    name: None,
                }],
                options: InfraBatchOptions::default(),
            },
        };

        let result = validate_batch_config(&config);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("apiVersion"));
    }

    #[test]
    fn test_validate_batch_config_invalid_kind() {
        let config = InfraBatchConfig {
            api_version: "pmp.io/v1".to_string(),
            kind: "WrongKind".to_string(),
            metadata: InfraBatchMetadata {
                name: "test".to_string(),
            },
            spec: InfraBatchSpec {
                provider: "aws".to_string(),
                regions: vec![],
                destination: InfraBatchDestination {
                    dest_type: "new_project".to_string(),
                    project_name: "my-project".to_string(),
                    environment: "production".to_string(),
                },
                resources: vec![InfraBatchResource {
                    resource_type: "aws_vpc".to_string(),
                    id: "vpc-12345".to_string(),
                    name: None,
                }],
                options: InfraBatchOptions::default(),
            },
        };

        let result = validate_batch_config(&config);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("kind"));
    }

    #[test]
    fn test_validate_batch_config_empty_resources() {
        let config = InfraBatchConfig {
            api_version: "pmp.io/v1".to_string(),
            kind: "InfrastructureImport".to_string(),
            metadata: InfraBatchMetadata {
                name: "test".to_string(),
            },
            spec: InfraBatchSpec {
                provider: "aws".to_string(),
                regions: vec![],
                destination: InfraBatchDestination {
                    dest_type: "new_project".to_string(),
                    project_name: "my-project".to_string(),
                    environment: "production".to_string(),
                },
                resources: vec![],
                options: InfraBatchOptions::default(),
            },
        };

        let result = validate_batch_config(&config);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("No resources"));
    }

    #[test]
    fn test_validate_batch_config_empty_project_name() {
        let config = InfraBatchConfig {
            api_version: "pmp.io/v1".to_string(),
            kind: "InfrastructureImport".to_string(),
            metadata: InfraBatchMetadata {
                name: "test".to_string(),
            },
            spec: InfraBatchSpec {
                provider: "aws".to_string(),
                regions: vec![],
                destination: InfraBatchDestination {
                    dest_type: "new_project".to_string(),
                    project_name: "".to_string(),
                    environment: "production".to_string(),
                },
                resources: vec![InfraBatchResource {
                    resource_type: "aws_vpc".to_string(),
                    id: "vpc-12345".to_string(),
                    name: None,
                }],
                options: InfraBatchOptions::default(),
            },
        };

        let result = validate_batch_config(&config);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("project_name"));
    }

    #[test]
    fn test_build_batch_destination_new_project() {
        let dest = InfraBatchDestination {
            dest_type: "new_project".to_string(),
            project_name: "my-project".to_string(),
            environment: "production".to_string(),
        };

        let result = build_batch_destination(&dest).unwrap();

        match result {
            ImportDestination::NewProject {
                project_name,
                environment,
            } => {
                assert_eq!(project_name, "my-project");
                assert_eq!(environment, "production");
            }
            _ => panic!("Expected NewProject"),
        }
    }

    #[test]
    fn test_build_batch_destination_existing_project() {
        let dest = InfraBatchDestination {
            dest_type: "existing_project".to_string(),
            project_name: "my-project".to_string(),
            environment: "staging".to_string(),
        };

        let result = build_batch_destination(&dest).unwrap();

        match result {
            ImportDestination::ExistingProject {
                project_name,
                environment,
            } => {
                assert_eq!(project_name, "my-project");
                assert_eq!(environment, "staging");
            }
            _ => panic!("Expected ExistingProject"),
        }
    }

    #[test]
    fn test_build_batch_destination_invalid_type() {
        let dest = InfraBatchDestination {
            dest_type: "invalid_type".to_string(),
            project_name: "my-project".to_string(),
            environment: "production".to_string(),
        };

        let result = build_batch_destination(&dest);
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("Invalid destination type"));
    }
}
